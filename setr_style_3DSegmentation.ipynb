{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNq3XI77LnUfUd3/J0FcZuT"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1cZB3t2qDXh",
        "outputId": "ab79b753-f9b3-4748-b736-e2c0be40d3dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit-pytorch\n",
            "  Downloading vit_pytorch-1.11.7-py3-none-any.whl.metadata (69 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from vit-pytorch) (0.8.1)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.12/dist-packages (from vit-pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from vit-pytorch) (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->vit-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->vit-pytorch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10->vit-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10->vit-pytorch) (3.0.2)\n",
            "Downloading vit_pytorch-1.11.7-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vit-pytorch\n",
            "Successfully installed vit-pytorch-1.11.7\n"
          ]
        }
      ],
      "source": [
        "! pip install vit-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from vit_pytorch.simple_vit_3d import SimpleViT\n",
        "from vit_pytorch.simple_vit_3d import posemb_sincos_3d\n",
        "from einops import rearrange\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ],
      "metadata": {
        "id": "jaoYynsYsZs0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_3d = SimpleViT(\n",
        "    image_size = 128,          # image size\n",
        "    frames = 16,               # number of frames\n",
        "    image_patch_size = 16,     # image patch size\n",
        "    frame_patch_size = 2,      # frame patch size\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 2048\n",
        ")\n",
        "\n",
        "#example input:\n",
        "#video = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\n",
        "\n",
        "#embedding size will be (4, 1000)\n",
        "#preds = vit_3d(video) # (4, 1000)"
      ],
      "metadata": {
        "id": "vMXDksyerLwT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleDeconv3DBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes):\n",
        "        super().__init__()\n",
        "        self.block = nn.ConvTranspose3d(in_planes, out_planes, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class SingleConv3DBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size):\n",
        "        super().__init__()\n",
        "        self.block = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
        "                               padding=((kernel_size - 1) // 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class Conv3DBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            SingleConv3DBlock(in_planes, out_planes, kernel_size),\n",
        "            nn.BatchNorm3d(out_planes),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class Deconv3DBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            SingleDeconv3DBlock(in_planes, out_planes),\n",
        "            SingleConv3DBlock(out_planes, out_planes, kernel_size),\n",
        "            nn.BatchNorm3d(out_planes),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ],
      "metadata": {
        "id": "bqBvmozAsuy5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleViT3dSeg(SimpleViT):\n",
        "  #add decoder attributes for segmentation\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "    print(kwargs['dim'], kwargs['num_classes'])\n",
        "    #upsampling decoder, from patch size 8x8x8 to 128x128x128\n",
        "    self.decoder = nn.Sequential(\n",
        "      Deconv3DBlock(kwargs['dim'], 256),\n",
        "      Deconv3DBlock(256, 128),\n",
        "      Deconv3DBlock(128, 64),\n",
        "      nn.Conv3d(in_channels=64, out_channels=kwargs['num_classes'], kernel_size=1)\n",
        "    )\n",
        "\n",
        "  #override the forward function so embeddings get fed into decoder instead of linear classification head\n",
        "  def forward(self, video):\n",
        "        *_, h, w, dtype = *video.shape, video.dtype\n",
        "\n",
        "        x = self.to_patch_embedding(video)\n",
        "        print(f\"patch shape: {x.shape}\")\n",
        "        _, depth_patch_size, height_patch_size, width_patch_size, _ = x.shape\n",
        "\n",
        "        pe = posemb_sincos_3d(x)\n",
        "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        print(f\"raw embeddings shape: {x.shape}\")\n",
        "        batch_size, patch_volume, embd_size = x.shape\n",
        "\n",
        "        #convert to per-patch embedding format for segmentation\n",
        "        #credit: code for re-arranging to per-patch format generated from chatgpt\n",
        "        feat_grid_embeddings = x.transpose(1, 2).contiguous().view(batch_size, embd_size, depth_patch_size, height_patch_size, width_patch_size)\n",
        "        print(f\"per-patch embedding dim: {feat_grid_embeddings.shape}\")\n",
        "\n",
        "        #x = x.mean(dim = 1) #don't apply pooling since we want the per-patch embedding\n",
        "        feat_grid_embeddings = self.to_latent(feat_grid_embeddings) #this is a palce holder, does nothing so we can keep it\n",
        "\n",
        "        logits = self.decoder(feat_grid_embeddings)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "raIJ8Zr5rshl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_vit_seg_model = SimpleViT3dSeg(\n",
        "    image_size = 64,          # image size\n",
        "    frames = 64,               # for volumetric data: this is slice number/depth\n",
        "    image_patch_size = 8,     # image patch size\n",
        "    frame_patch_size = 8,      # for volumetric data: this should be same as image patch size\n",
        "    num_classes = 3,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 2048,\n",
        ")\n",
        "\n",
        "test_input = torch.randn(1, 3, 64, 64, 64)\n",
        "\n",
        "preds = test_vit_seg_model(test_input)\n",
        "\n",
        "print(preds.shape, type(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHtpFeiJy1r9",
        "outputId": "204fd372-a9a1-4aeb-91cf-744c7ebc4c30"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024 3\n",
            "patch shape: torch.Size([1, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([1, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([1, 1024, 8, 8, 8])\n",
            "torch.Size([1, 3, 64, 64, 64]) <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "vit_3d_seg = SimpleViT3dSeg(\n",
        "    image_size = 64,\n",
        "    frames = 64,\n",
        "    image_patch_size = 8,\n",
        "    frame_patch_size = 8,\n",
        "    num_classes = 3,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 2048,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 5\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "vit_3d_seg.to(device)\n",
        "optimizer = optim.Adam(vit_3d_seg.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "\n",
        "\n",
        "#dummy data\n",
        "X = torch.randn(8, 3, 64, 64, 64)            # (B, C, T, H, W)\n",
        "Y = torch.randint(0, 3, (8, 64, 64, 64))     # (B, T, H, W) with 3 classes\n",
        "train_loader = DataLoader(TensorDataset(X, Y), batch_size=2, shuffle=True)\n",
        "\n",
        "for ep in range(epochs):\n",
        "  vit_3d_seg.train()\n",
        "\n",
        "  for x, y in train_loader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logits = vit_3d_seg(x)\n",
        "    loss = ce_loss(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"epoch: {ep}, loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9uDVIbVz6qa",
        "outputId": "1daadc78-3191-4a67-81a1-cc92c3236a3b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024 3\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "epoch: 0, loss: 1.1150\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "epoch: 1, loss: 1.1035\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "epoch: 2, loss: 1.1009\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "epoch: 3, loss: 1.1004\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "patch shape: torch.Size([2, 8, 8, 8, 1024])\n",
            "raw embeddings shape: torch.Size([2, 512, 1024])\n",
            "per-patch embedding dim: torch.Size([2, 1024, 8, 8, 8])\n",
            "epoch: 4, loss: 1.0993\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_tF2qOkzG9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}