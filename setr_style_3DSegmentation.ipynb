{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPW3vi3dAtqcRWcXitRx4Xe"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1cZB3t2qDXh",
        "outputId": "ab79b753-f9b3-4748-b736-e2c0be40d3dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vit-pytorch\n",
            "  Downloading vit_pytorch-1.11.7-py3-none-any.whl.metadata (69 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: einops>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from vit-pytorch) (0.8.1)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.12/dist-packages (from vit-pytorch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from vit-pytorch) (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.10->vit-pytorch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->vit-pytorch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->vit-pytorch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.10->vit-pytorch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.10->vit-pytorch) (3.0.2)\n",
            "Downloading vit_pytorch-1.11.7-py3-none-any.whl (142 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vit-pytorch\n",
            "Successfully installed vit-pytorch-1.11.7\n"
          ]
        }
      ],
      "source": [
        "! pip install vit-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from vit_pytorch.simple_vit_3d import SimpleViT\n",
        "from vit_pytorch.simple_vit_3d import posemb_sincos_3d\n",
        "from einops import rearrange\n",
        "from PIL import Image\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n"
      ],
      "metadata": {
        "id": "jaoYynsYsZs0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_3d = SimpleViT(\n",
        "    image_size = 128,          # image size\n",
        "    frames = 16,               # number of frames\n",
        "    image_patch_size = 16,     # image patch size\n",
        "    frame_patch_size = 2,      # frame patch size\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 2048\n",
        ")\n",
        "\n",
        "#example input:\n",
        "#video = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\n",
        "\n",
        "#embedding size will be (4, 1000)\n",
        "#preds = vit_3d(video) # (4, 1000)"
      ],
      "metadata": {
        "id": "vMXDksyerLwT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#source code for these conv block implemenations is from this UNETR implementation: https://github.com/tamasino52/UNETR/blob/main/unetr.py#L231\n",
        "class SingleDeconv3DBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes):\n",
        "        super().__init__()\n",
        "        self.block = nn.ConvTranspose3d(in_planes, out_planes, kernel_size=2, stride=2, padding=0, output_padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class SingleConv3DBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size):\n",
        "        super().__init__()\n",
        "        self.block = nn.Conv3d(in_planes, out_planes, kernel_size=kernel_size, stride=1,\n",
        "                               padding=((kernel_size - 1) // 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class Conv3DBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            SingleConv3DBlock(in_planes, out_planes, kernel_size),\n",
        "            nn.BatchNorm3d(out_planes),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "\n",
        "class Deconv3DBlock(nn.Module):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3):\n",
        "        super().__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            SingleDeconv3DBlock(in_planes, out_planes),\n",
        "            SingleConv3DBlock(out_planes, out_planes, kernel_size),\n",
        "            nn.BatchNorm3d(out_planes),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)"
      ],
      "metadata": {
        "id": "bqBvmozAsuy5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleViT3dSeg(SimpleViT):\n",
        "  #add decoder attributes for segmentation\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "\n",
        "    print(kwargs['dim'], kwargs['num_classes'])\n",
        "    #upsampling decoder, from patch size 8x8x8 to 128x128x128\n",
        "    self.decoder = nn.Sequential(\n",
        "      Deconv3DBlock(kwargs['dim'], 256),\n",
        "      Deconv3DBlock(256, 128),\n",
        "      Deconv3DBlock(128, 64),\n",
        "      nn.Conv3d(in_channels=64, out_channels=kwargs['num_classes'], kernel_size=1)\n",
        "    )\n",
        "\n",
        "  #override the forward function so embeddings get fed into decoder instead of linear classification head\n",
        "  def forward(self, video):\n",
        "        *_, h, w, dtype = *video.shape, video.dtype\n",
        "\n",
        "        x = self.to_patch_embedding(video)\n",
        "        #print(f\"patch shape: {x.shape}\")\n",
        "        _, depth_patch_size, height_patch_size, width_patch_size, _ = x.shape\n",
        "\n",
        "        pe = posemb_sincos_3d(x)\n",
        "        x = rearrange(x, 'b ... d -> b (...) d') + pe\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        #print(f\"raw embeddings shape: {x.shape}\")\n",
        "        batch_size, patch_volume, embd_size = x.shape\n",
        "\n",
        "        #convert to per-patch embedding format for segmentation\n",
        "        #credit: code for re-arranging to per-patch format generated from chatgpt\n",
        "        feat_grid_embeddings = x.transpose(1, 2).contiguous().view(batch_size, embd_size, depth_patch_size, height_patch_size, width_patch_size)\n",
        "        #print(f\"per-patch embedding dim: {feat_grid_embeddings.shape}\")\n",
        "\n",
        "        #x = x.mean(dim = 1) #don't apply pooling since we want the per-patch embedding\n",
        "        feat_grid_embeddings = self.to_latent(feat_grid_embeddings) #this is a palce holder, does nothing so we can keep it\n",
        "\n",
        "        logits = self.decoder(feat_grid_embeddings)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "raIJ8Zr5rshl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_vit_seg_model = SimpleViT3dSeg(\n",
        "    image_size = 64,          # image size\n",
        "    frames = 64,               # for volumetric data: this is slice number/depth\n",
        "    image_patch_size = 8,     # image patch size\n",
        "    frame_patch_size = 8,      # for volumetric data: this should be same as image patch size\n",
        "    num_classes = 3,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 2048,\n",
        ")\n",
        "\n",
        "test_input = torch.randn(1, 3, 64, 64, 64)\n",
        "\n",
        "preds = test_vit_seg_model(test_input)\n",
        "\n",
        "print(preds.shape, type(preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHtpFeiJy1r9",
        "outputId": "3d13d354-2b9d-43cd-ee7f-a6f4e5277e1d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024 3\n",
            "torch.Size([1, 3, 64, 64, 64]) <class 'torch.Tensor'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#chatgpt generated using prompt \"generate a simple function that generates a 3 channel white cube, inside it is a red sphere of varied position and radius\"\n",
        "\n",
        "def make_rgb_cube_with_red_sphere(T=64, H=64, W=64, cube_margin=8,\n",
        "                                  min_radius=4, device=\"cpu\"):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      img:  (3, T, H, W) float32 in [0,1]\n",
        "      mask: (T, H, W)    long: 0=bg, 1=cube, 2=sphere\n",
        "    \"\"\"\n",
        "    # --- init ---\n",
        "    img  = torch.zeros(3, T, H, W, dtype=torch.float32, device=device)  # RGB\n",
        "    mask = torch.zeros(T, H, W, dtype=torch.long, device=device)        # seg labels\n",
        "\n",
        "    # --- define cube bounds (axis-aligned) ---\n",
        "    z0, z1 = cube_margin, T - cube_margin\n",
        "    y0, y1 = cube_margin, H - cube_margin\n",
        "    x0, x1 = cube_margin, W - cube_margin\n",
        "\n",
        "    # white cube\n",
        "    img[:, z0:z1, y0:y1, x0:x1] = 1.0\n",
        "    mask[  z0:z1, y0:y1, x0:x1] = 1\n",
        "\n",
        "    # --- random sphere INSIDE the cube ---\n",
        "    # max radius so sphere fits fully inside cube\n",
        "    max_r = min((z1 - z0), (y1 - y0), (x1 - x0)) // 3\n",
        "    r = int(torch.randint(min_radius, max( min_radius+1, max_r+1 ), ()).item())\n",
        "\n",
        "    cz = int(torch.randint(z0 + r, z1 - r, ()).item())\n",
        "    cy = int(torch.randint(y0 + r, y1 - r, ()).item())\n",
        "    cx = int(torch.randint(x0 + r, x1 - r, ()).item())\n",
        "\n",
        "    # build sphere mask\n",
        "    zz = torch.arange(T, device=device).view(T, 1, 1)\n",
        "    yy = torch.arange(H, device=device).view(1, H, 1)\n",
        "    xx = torch.arange(W, device=device).view(1, 1, W)\n",
        "    sphere = (zz - cz)**2 + (yy - cy)**2 + (xx - cx)**2 <= r**2\n",
        "\n",
        "    # paint sphere: red on top of cube\n",
        "    img[0][sphere] = 1.0   # R\n",
        "    img[1][sphere] = 0.0   # G\n",
        "    img[2][sphere] = 0.0   # B\n",
        "    mask[sphere]    = 2\n",
        "\n",
        "    return img.contiguous(), mask.contiguous()\n",
        "\n",
        "#chatgpt generated using prompt \"generate a function to visualize the generated data\"\n",
        "def save_gif(img, path=\"volume.gif\"):\n",
        "    _, T, _, _ = img.shape\n",
        "    frames = []\n",
        "    for i in range(T):\n",
        "        slice_rgb = (img[:, i].permute(1,2,0).cpu().numpy() * 255).astype(\"uint8\")\n",
        "        frames.append(Image.fromarray(slice_rgb))\n",
        "    frames[0].save(path, save_all=True, append_images=frames[1:], duration=100, loop=0)"
      ],
      "metadata": {
        "id": "AL4qQdBf8aaG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generate dataset\n",
        "train_img = []\n",
        "train_mask = []\n",
        "test_img = []\n",
        "test_mask = []\n",
        "for i in range(100):\n",
        "  img, mask = make_rgb_cube_with_red_sphere(device=\"cuda\")\n",
        "  train_img.append(img)\n",
        "  train_mask.append(mask)\n",
        "\n",
        "for i in range(30):\n",
        "  img, mask = make_rgb_cube_with_red_sphere(device=\"cuda\")\n",
        "  test_img.append(img)\n",
        "  test_mask.append(mask)\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(torch.stack(train_img), torch.stack(train_mask)), batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(TensorDataset(torch.stack(test_img), torch.stack(test_mask)), batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "IiaPUJ-g_Rto"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, mask = make_rgb_cube_with_red_sphere(device=\"cuda\")\n",
        "save_gif(img, \"volume.gif\")"
      ],
      "metadata": {
        "id": "ihnsH0UgA9ER"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training loop\n",
        "vit_3d_seg = SimpleViT3dSeg(\n",
        "    image_size = 64,\n",
        "    frames = 64,\n",
        "    image_patch_size = 8,\n",
        "    frame_patch_size = 8,\n",
        "    num_classes = 3,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 8,\n",
        "    mlp_dim = 2048,\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "epochs = 100\n",
        "ce_loss = nn.CrossEntropyLoss()\n",
        "vit_3d_seg.to(device)\n",
        "optimizer = optim.Adam(vit_3d_seg.parameters(), lr=1e-3, weight_decay=1e-2)\n",
        "\n",
        "for ep in range(epochs):\n",
        "  vit_3d_seg.train()\n",
        "  train_loss = 0.0\n",
        "\n",
        "  for x, y in train_loader:\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    logits = vit_3d_seg(x)\n",
        "    loss = ce_loss(logits, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "  train_loss = train_loss / len(train_loader)\n",
        "\n",
        "  #eval\n",
        "  vit_3d_seg.eval()\n",
        "  test_loss = 0.0\n",
        "  with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "      x, y = x.to(device), y.to(device)\n",
        "      logits = vit_3d_seg(x)\n",
        "      loss = ce_loss(logits, y)\n",
        "      test_loss += loss.item()\n",
        "    test_loss = test_loss / len(test_loader)\n",
        "\n",
        "  print(f\"epoch: {ep}, train loss: {train_loss:.4f}, test loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9uDVIbVz6qa",
        "outputId": "4765ff93-10a4-47e0-d709-12222def5471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024 3\n",
            "epoch: 0, train loss: 0.6391, test loss: 12.1639\n",
            "epoch: 1, train loss: 0.3359, test loss: 1.2868\n",
            "epoch: 2, train loss: 0.2821, test loss: 1.8189\n",
            "epoch: 3, train loss: 0.2135, test loss: 2.2810\n",
            "epoch: 4, train loss: 0.1847, test loss: 0.4686\n",
            "epoch: 5, train loss: 0.1167, test loss: 1.4949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4_tF2qOkzG9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}